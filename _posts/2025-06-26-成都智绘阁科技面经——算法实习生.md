---
layout: post
title: "成都智绘阁科技面经——算法实习生"
date: 2025-06-26
description: "面试成都智绘阁科技的算法实习生岗位"
tag: 面经
---   

　　大四末找实习真难，兜兜转转面了好几个，只过了两个数据标注，但是含金量太低了，没有刷简历的作用，已拒。这个算法实习岗位还是比较合适的，我对AI大模型也很有兴趣。路程要一个半小时，希望能通过。
	更：没通过
 

## 背景

薪资：200-500/天
职位描述：
	1、投身算法落地之实践，参与系统对接之事宜。依托既有平台，实现大规模语言模型于具体任务之应用，协同开发人员，完成功能对接与调适工作。
	2、于工作进程中，基于对算法之理解，主动探寻新应 用场景，积极提出创新构想，并付诸原型实现。
	3、辅助算法工程师及内容助理，推动AI算法在电商领 域之应用落地。
	4、致力于prompt之开发与测试工作。
任职要求：
	1、全日制统招重点本科（一本以上）或硕士以上，人工智能，计算机，空间大数据，数学等相关专业，要求聪明，好学，懂prompt engineering。
	2、有NLP领域相关开发或科研经验，了解NLP领域重 要任务和主流模型（Bert，GPT），对预训练语言模 型有深入了解；
	3、对ai产品感兴趣，了解主流ai软件。
	4、良好的沟通能力和团队协作能力，自我驱动型工作态度，对大模型的应用感兴趣，持续学习与技术创新意识。
 
## 面试问到的问题

### 1. 评论有效性排序项目的流程      

具体讲述这个网络如何进行分类或排序，需要了解网络具体结构。

### 2. 评论有效性排序项目使用的是（CNN）卷积神经网络吗？

不是，是RNN的一种。

### 3. LSTM模型是什么模型的改进？

是RNN（循环神经网络）参考[LSTM从入门到精通](https://blog.csdn.net/mary19831/article/details/129570030)。

### 4. 评价模型如何进行数据清洗

以下是用于清理数据的清单:

删除所有不相关的字符，例如任何非字母数字字符；
令牌化通过将其分割成单个的单词文本；
删除不相关的单词，例如“@”twitter提及或网址；
将所有字符转换为小写，以便将诸如“hello”，“Hello”和“HELLO”之类的单词视为相同；
考虑将拼写错误或交替拼写的单词组合成单个表示（例如“cool”/“kewl”/“cooool”）；
考虑词开还原（将诸如“am”，“are”和“is”之类的词语简化为诸如“be”之类的常见形式）；

参考对NLP的介绍：[[NLP] 自然语言处理 --- NLP入门指南](https://blog.csdn.net/zwqjoy/article/details/103546648)

### 5. bert和gpt的区别，对比它们的预训练模型

BERT：双向 预训练语言模型+fine-tuning（微调），BERT预训练过程中采用了双向预测的方法，即通过预测句子中丢失的词来学习语言模型。

GPT：自回归 预训练语言模型+Prompting（指示/提示），GPT预训练过程中，采用了语言模型的方法，即通过预测下一个词来学习语言模型。

具体可参考[Transformer两大变种：GPT和BERT的差别](https://www.zhihu.com/tardis/zm/art/607605399)和[万字长文，带你搞懂什么是BERT模型（非常详细）看这一篇就够了！](https://blog.csdn.net/star_nwe/article/details/143227601)。

### 6. 注意力机制的数学表达式

注意力分数*值矩阵，具体参考[Attention注意力机制的公式解析](https://blog.csdn.net/Zlyzjiabjw547479/article/details/145065379)。

### 7. bert模型，怎样对矩阵进行mask（即给你一个句子，怎样把后面的词掩码，只留下前面的词）？

没答出来，HR给的答案：词汇拆分后，分别转换成向量表示，对需要掩码的词的向量中的值置为负无穷，那么这部分值在进行softmax后就会归零，使其对结果不产生影响。

### 8. prompt engineering

参考[Prompt Engineering简介](https://www.bing.com/ck/a?!&&p=42b3457da00fdf0387fe230ba8770868b1fcdbcf789835b98c1b3714468fc403JmltdHM9MTc1MDg5NjAwMA&ptn=3&ver=2&hsh=4&fclid=1d561086-f6c3-6b1c-214a-0683f7116a8c&psq=prompt+engineering%e4%bb%8b%e7%bb%8d&u=a1aHR0cHM6Ly9nZWVrZGF4dWUuY28vcmVhZC9jaGVuamlhbkBwcm9tcHQva3BxZWRidGlic2QyMGNpdA&ntb=1)。

### 9. Word2vec 的两种训练模式

CBOW(Continuous Bag-of-Words Model)和Skip-gram (Continuous Skip-gram Model)，是Word2vec 的两种训练模式。参考[一文看懂 Word2vec（基本概念+2种训练模型+5个优缺点）](https://zhuanlan.zhihu.com/p/84301849)。

## 一点感悟

> 同样是计算机，同样是AI，也是隔行如隔山，这个做nlp的就不问cv内容，面试可以重点准备；
> 看重自我学习能力与干劲，要聪明人。
> 对找工作来说，最重要的两点：学历；项目经历与应聘岗位匹配度。其实要找算法、开发或者大模型等计算机相关工作，自己做的项目其实能理解透彻、工作量够且有意义、与应聘岗位相关就可以，实习不是必须的，但有的话更好。要早做打算，真正提升自己对于要从事行业的理解与此领域的能力才最重要。
> 可以尽可能地确定一个或几个小方向，因为隔行如隔山，比如做CV或者NLP。

<p> </p>

